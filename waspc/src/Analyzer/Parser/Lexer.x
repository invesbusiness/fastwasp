{
-- This file is processed by Alex (https://www.haskell.org/alex/) and generates
-- the module `Analyzer.Parser.Lexer`

{-# LANGUAGE NamedFieldPuns #-}

module Analyzer.Parser.Lexer
  ( lexer
  ) where

import Analyzer.Parser.Monad (ParserInput, Parser, ParserState (..), updatePosition, putInput, setStartCode)
import Analyzer.Parser.Token (Token (..), TokenClass (..))
import Analyzer.Parser.ParseError (ParseError (..))
import Control.Monad.State.Lazy (get)
import Control.Monad.Except (throwError)
import Data.Word (Word8)
import Codec.Binary.UTF8.String (encodeChar)
}

-- Character set aliases
$digit = 0-9
$alpha = [a-zA-Z]
$identstart = [_$alpha]
$ident = [_$alpha$digit]
$any = [.$white]

-- Regular expression aliases
@string = \"([^\\\"]|\\.)*\" -- matches string-literal on a single line, from https://stackoverflow.com/a/9260547/3902376
@double = "-"? $digit+ "." $digit+
@integer = "-"? $digit+
@ident = $identstart $ident* "'"*

-- Tokenization rules (regex -> token)
tokens :-

-- Skips whitespace
<0>       $white+ ;

-- Quoter rules:
-- Uses Alex start codes to lex quoted characters with different rules:
-- - On "{=tag", enter <quoter> start code and make a TLQuote token
-- - While in <quoter>, if "tag=}" is seen, enter <0> (default) start code and make a TRQuote token
-- - Otherwise, take one character at a time and make a TQuoted token
<0>       "{=" @ident { beginQuoter }
<quoter>  @ident "=}" { endQuoter }
<quoter>  $any { createValueToken TQuoted }

-- Simple tokens
<0>       "{" { createConstToken TLCurly }
<0>       "}" { createConstToken TRCurly }
<0>       "," { createConstToken TComma }
<0>       ":" { createConstToken TColon }
<0>       "[" { createConstToken TLSquare }
<0>       "]" { createConstToken TRSquare }
<0>       "import" { createConstToken TImport }
<0>       "from" { createConstToken TFrom }
<0>       "true" { createConstToken TTrue }
<0>       "false" { createConstToken TFalse }

-- Strings, numbers, identifiers
<0>       @string { createValueToken $ \s -> TString $ read s }
<0>       @double { createValueToken $ \s -> TDouble $ read s }
<0>       @integer { createValueToken $ \s -> TInt $ read s }
<0>       @ident { createValueToken $ \s -> TIdentifier s }

{

-- Alex needs the input type to be called "AlexInput"
type AlexInput = ParserInput

-- | Required by Alex.
--
--   This function is taken from the Alex basic wrapper.
alexGetByte :: AlexInput -> Maybe (Word8, AlexInput)
alexGetByte (c, (b:bs), s) = Just (b, (c, bs, s))
alexGetByte (_, [], []) = Nothing
alexGetByte (_, [], (c:s)) = case encodeChar c of
                               (b:bs) -> Just (b, (c, bs, s))
                               [] -> Nothing

-- | Required by Alex.
--
--   This function is taken from the Alex basic wrapper.
alexInputPrevChar :: AlexInput -> Char
alexInputPrevChar (c, _, _) = c

-- | Lexes a single token from the input.
--
--   This function is designed for use with the Happy monadic parser that uses threaded/monadic lexer.
--   This means that parser, as it is building an AST, asks for a single token at a time from the lexer, on the go.
--   This is done in "continuation" style -> parser calls lexer while passing it the function ('parseToken') via which
--   lexer gives control back to the parser.
--   In such setup both lexer and parser are operating in the same 'Parser' monad.
--   Check https://www.haskell.org/happy/doc/html/sec-monads.html#sec-lexers for more details.
--
--   This function internally calls `alexScan`, which is a function generated by Alex responsible for doing actual lexing/scanning.
lexer :: (Token -> Parser a) -> Parser a
lexer parseToken = do
  input@(previousChar, _, remainingSource) <- parserRemainingInput <$> get
  startCode <- parserStartCode <$> get
  case alexScan input startCode of
    AlexEOF -> do
      createConstToken TEOF "" >>= parseToken
    AlexError _ -> do
      pos <- parserSourcePosition <$> get
      throwError $ UnexpectedChar previousChar pos
    AlexSkip input' numCharsSkipped -> do
      updatePosition $ take numCharsSkipped remainingSource
      putInput input'
      lexer parseToken
    AlexToken input' tokenLength action -> do
      -- Token is made before `updatePosition` so that its `tokenPosition` points to
      -- the start of the token's lexeme.
      token <- action $ take tokenLength remainingSource
      updatePosition $ take tokenLength remainingSource
      putInput input'
      parseToken token

-- | Takes a lexeme like "{=json" and sets the quoter start code
beginQuoter :: String -> Parser Token
beginQuoter leftQuoteTag = do
  setStartCode quoter
  let tag = drop 2 leftQuoteTag
  createConstToken (TLQuote tag) leftQuoteTag

-- | Takes a lexeme like "json=}" and returns to start code 0
endQuoter :: String -> Parser Token
endQuoter rightQuoteTag = do
  setStartCode 0
  let tag = take (length rightQuoteTag - 2) rightQuoteTag
  createConstToken (TRQuote tag) rightQuoteTag

-- | Makes an action that creates a token from a constant TokenClass.
createConstToken :: TokenClass -> (String -> Parser Token)
createConstToken tc lexeme = do
  position <- parserSourcePosition <$> get
  return $ Token { tokenClass = tc
                 , tokenPosition = position
                 , tokenLexeme = lexeme
                 }

-- | Makes an action that creates a token using the input lexeme.
createValueToken :: (String -> TokenClass) -> (String -> Parser Token)
createValueToken getTokenClass lexeme = createConstToken (getTokenClass lexeme) lexeme
}
